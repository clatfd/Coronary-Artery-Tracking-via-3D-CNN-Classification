{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for artery tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import datetime\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskdir = '//Desktop4/Dtensorflow\\\\LiChen\\\\AICafe\\\\CNNTracker'\n",
    "taskname = 'CNNTracker1-1'\n",
    "if not os.path.exists(taskdir+'/'+taskname):\n",
    "    os.mkdir(taskdir+'/'+taskname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'U:\\LiChen\\AICafe\\CNNTracker')\n",
    "from models.centerline_net import CenterlineNet\n",
    "from centerline_train_tools.data_provider_argu import DataGenerater\n",
    "from centerline_train_tools.centerline_trainner import Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import iCafe Python\n",
    "import numpy as np\n",
    "import sys\n",
    "#sys.path.append(r'\\\\DESKTOP2\\Ftensorflow\\LiChen\\iCafe')\n",
    "sys.path.insert(0,r'\\\\DESKTOP4\\Dtensorflow\\LiChen\\iCafePython')\n",
    "from iCafePython import iCafe\n",
    "from iCafePython import SnakeList,Snake,SWCNode,Point3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbname = 'BRAVEAI'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "with open(r\"\\\\DESKTOP2\\GiCafe\\result\\BRAVEAI\\db.list\",'rb') as fp:\n",
    "    dblist = pickle.load(fp)\n",
    "train_list = dblist['train']\n",
    "val_list = dblist['val']\n",
    "test_list = dblist['test']\n",
    "pilist = [pi.split('/')[1] for pi in dblist['test']]\n",
    "len(pilist),pilist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'RotterdanCoronary'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "pilist = ['0_dataset03_U']\n",
    "icafem = iCafe(icafe_dir+'/'+pilist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'UNC'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "with open(icafe_dir+'/db.list','rb') as fp:\n",
    "    dblist = pickle.load(fp)\n",
    "pilist = [pi.split('/')[1] for pi in dblist['test']]\n",
    "len(pilist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'HarborViewT1Pre'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "\n",
    "pilist = ['0_ID%d_U'%i for i in [2,9,10,11,12]]\n",
    "len(pilist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE \n",
    "dbname = 'CAREIIMERGEGT'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "with open(icafe_dir+'/db.list','rb') as fp:\n",
    "    dblist = pickle.load(fp)\n",
    "pilist = [pi.split('/')[1] for pi in dblist['test']]\n",
    "len(pilist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPH supp\n",
    "dbname = 'IPH-Sup-TOF-FullCoverage'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "\n",
    "dblist_name = icafe_dir+'/db.list'\n",
    "\n",
    "with open(dblist_name,'rb') as fp:\n",
    "    dblist = pickle.load(fp)\n",
    "        \n",
    "pilist = [pi.split('/')[1] for pi in dblist['test']]\n",
    "len(pilist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilist = [pi.split('/')[1] for pi in dblist['train']]+[pi.split('/')[1] for pi in dblist['val']]+[pi.split('/')[1] for pi in dblist['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pi in pilist[:]:\n",
    "    tif_name = icafe_dir + '/' + pi + '/TH_' + pi + '.npy'\n",
    "    if 1:\n",
    "        icafem = iCafe(icafe_dir +'/'+pi)\n",
    "        icafem.loadImg('o')\n",
    "        \n",
    "\n",
    "        tif_norm_img = copy.copy(icafem.tifimg)\n",
    "        tif_norm_img = tif_norm_img/1000\n",
    "        print(pi,np.max(tif_norm_img))\n",
    "        np.save(icafem.getPath('.npy'), tif_norm_img.astype(np.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance transfrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for pi in pilist[:]:\n",
    "    \n",
    "    icafem = iCafe(icafe_dir +'/'+pi)\n",
    "    if icafem.existPath('d.npy'):\n",
    "        continue\n",
    "    print(pi)\n",
    "    #ves\n",
    "    #snakelist = icafem.vessnakelist\n",
    "    #raw_ves\n",
    "    snakelist = icafem.snakelist\n",
    "    icafem.paintDistTransform(snakelist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(train_patch_list,val_patch_list):\n",
    "    '''\n",
    "    :return: train set,val set\n",
    "    '''\n",
    "    train_pre_fix_path = r\"D:\\LiChen\\BRAVEpatch\"\n",
    "    train_flag = 'train'\n",
    "    train_transforms = None\n",
    "    target_transform = None\n",
    "    train_dataset = DataGenerater(train_patch_list, train_pre_fix_path, 500, train_transforms, train_flag, target_transform)\n",
    "\n",
    "    val_pre_fix_path = r\"D:\\LiChen\\BRAVEpatch\"\n",
    "    val_flag = 'val'\n",
    "    test_valid_transforms = None\n",
    "    target_transform = None\n",
    "    val_dataset = DataGenerater(val_patch_list, val_pre_fix_path, 500, test_valid_transforms, val_flag, target_transform)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def cross_entropy(a, y):\n",
    "    epsilon = 1e-9\n",
    "    return torch.mean(torch.sum(-y * torch.log10(a + epsilon) - (1 - y) * torch.log10(1 - a + epsilon), dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_dir = r'D:\\LiChen\\BRAVEpatch\\icafe_patch\\offset\\point_500_gp_1'\n",
    "train_patch_list = []\n",
    "val_patch_list = []\n",
    "for pi in train_list:\n",
    "    if os.path.exists(list_dir+'/d'+pi.split('/')[1]+'_patch_info_500.csv'):\n",
    "        train_patch_list.append(list_dir+'/d'+pi.split('/')[1]+'_patch_info_500.csv')\n",
    "for pi in val_list:\n",
    "    if os.path.exists(list_dir+'/d'+pi.split('/')[1]+'_patch_info_500.csv'):\n",
    "        val_patch_list.append(list_dir+'/d'+pi.split('/')[1]+'_patch_info_500.csv')\n",
    "train_patch_list,val_patch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we use 8 fold cross validation, save_num means to use dataset0x as the validation set\n",
    "train_dataset, val_dataset = get_dataset(train_patch_list[:],val_patch_list[:])\n",
    "\n",
    "curr_model_name = \"centerline_net\"\n",
    "max_points = 500\n",
    "model = CenterlineNet(n_classes = max_points)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 16\n",
    "\n",
    "criterion = cross_entropy\n",
    "inital_lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=inital_lr,weight_decay=0.001)\n",
    "\n",
    "trainer = Trainer(batch_size,\n",
    "                  num_workers,\n",
    "                  train_dataset,\n",
    "                  val_dataset,\n",
    "                  model,\n",
    "                  curr_model_name,\n",
    "                  optimizer,\n",
    "                  criterion,\n",
    "                  max_points,\n",
    "                  start_epoch=38,\n",
    "                  max_epoch=100,\n",
    "                  initial_lr=inital_lr)\n",
    "trainer.run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(batch_size,\n",
    "                  num_workers,\n",
    "                  train_dataset,\n",
    "                  val_dataset,\n",
    "                  model,\n",
    "                  curr_model_name,\n",
    "                  optimizer,\n",
    "                  criterion,\n",
    "                  max_points,\n",
    "                  start_epoch=0,\n",
    "                  max_epoch=100,\n",
    "                  initial_lr=inital_lr)\n",
    "\n",
    "trainer.run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "data_path = r\"D:\\LiChen\\BRAVEpatch\\icafe_patch\\offset\\point_500_gp_1\\d0_7001_U\\d_0_7001_U_v_0_patch_1_0.nii.gz\"\n",
    "img = sitk.GetArrayFromImage(sitk.ReadImage(data_path, sitk.sitkFloat32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(img.reshape(19,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CNN tracker for ICA TOF MRA\n",
    "swc_name = 'cnn_snake'\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'U:\\LiChen\\AICafe\\CNNTracker')\n",
    "from models.centerline_net import CenterlineNet\n",
    "\n",
    "max_points = 500\n",
    "prob_thr = 0.85\n",
    "\n",
    "infer_model = CenterlineNet(n_classes=max_points)\n",
    "checkpoint_path_infer = r\"D:\\tensorflow\\LiChen\\AICafe\\CNNTracker\\CNNTracker1-1\\classification_checkpoints\\centerline_net_model_Epoch_29.pkl\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path_infer)\n",
    "net_dict = checkpoint['net_dict']\n",
    "infer_model.load_state_dict(net_dict)\n",
    "infer_model.to(device)\n",
    "infer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN tracker for Coronary CTA\n",
    "swc_name = 'cnn_snake'\n",
    "max_points = 500\n",
    "prob_thr = 0.85\n",
    "\n",
    "infer_model = CenterlineNet(n_classes=max_points)\n",
    "\n",
    "checkpoint_path_infer = r\"D:\\tensorflow\\LiChen\\AICafe\\CNNTracker\\CNNTracker2-1\\classification_checkpoints\\centerline_net_model_Epoch_81.pkl\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path_infer)\n",
    "net_dict = checkpoint['net_dict']\n",
    "infer_model.load_state_dict(net_dict)\n",
    "infer_model.to(device)\n",
    "infer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN tracker for LATTE\n",
    "swc_name = 'cnn_snake'\n",
    "max_points = 500\n",
    "prob_thr = 0.85\n",
    "\n",
    "infer_model = CenterlineNet(n_classes=max_points)\n",
    "checkpoint_path_infer = r\"D:\\tensorflow\\LiChen\\AICafe\\CNNTracker\\CNNTracker4-1\\classification_checkpoints\\centerline_net_model_Epoch_99.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path_infer)\n",
    "net_dict = checkpoint['net_dict']\n",
    "infer_model.load_state_dict(net_dict)\n",
    "infer_model.to(device)\n",
    "infer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DCAT for ICA TOF MRA\n",
    "swc_name = 'dcat_snake'\n",
    "\n",
    "from models.dcat_net import DCATNet\n",
    "checkpoint_path_infer = r\"D:\\tensorflow\\LiChen\\AICafe\\CNNTracker\\CNNTracker3-1\\classification_checkpoints\\DCAT_net_model_Epoch_7.pkl\"\n",
    "dcat_model = DCATNet(n_classes=max_points)\n",
    "checkpoint = torch.load(checkpoint_path_infer)\n",
    "net_dict = checkpoint['net_dict']\n",
    "dcat_model.load_state_dict(net_dict)\n",
    "\n",
    "max_points = 500\n",
    "prob_thr = 0.85\n",
    "\n",
    "infer_model = CenterlineNet(n_classes=max_points)\n",
    "\n",
    "model_dict = infer_model.state_dict()\n",
    "pretrained_dict = dcat_model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "infer_model.load_state_dict(pretrained_dict)\n",
    "\n",
    "infer_model.to(device)\n",
    "infer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'BRAVEAI'\n",
    "import SimpleITK as sitk\n",
    "from infer_tools_tree.build_vessel_tree import build_vessel_tree, TreeNode, dfs_search_tree\n",
    "\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/BRAVEAI/'\n",
    "for pi in pilist[:]:\n",
    "    print('='*10,'Start processing',pilist.index(pi),'/',len(pilist),pi,'='*10)\n",
    "\n",
    "    icafem = iCafe(icafe_dir+'/'+pi)\n",
    "    \n",
    "    #load image\n",
    "\n",
    "    file_name = icafem.getPath('o')\n",
    "    re_spacing_img = sitk.GetArrayFromImage(sitk.ReadImage(file_name))\n",
    "    \n",
    "    # generate/load seeds\n",
    "    from skimage.morphology import skeletonize,skeletonize_3d\n",
    "    from iCafePython.utils.img_utils import connectedCentroid\n",
    "    if icafem.existPath('con_seed.txt'):\n",
    "        con_seeds = np.loadtxt(icafem.getPath('con_seed.txt'))\n",
    "    else:\n",
    "        simg = icafem.loadImg('s.whole')>0.5\n",
    "        skeleton = skeletonize_3d(simg)\n",
    "        con_seeds = connectedCentroid(skeleton)\n",
    "        print('len',len(con_seeds))\n",
    "        np.savetxt(icafem.getPath('con_seed.txt'),con_seeds)\n",
    "\n",
    "    seeds = np.vstack(([i.intlst() for i in icafem.seeds], con_seeds))\n",
    "    print(len(seeds))\n",
    "    \n",
    "    #tracing\n",
    "    ostias = seeds[:1]\n",
    "    root = TreeNode(ostias, start_point_index=None)\n",
    "    all_traces = build_vessel_tree(infer_model, re_spacing_img, seeds[::], root=root)\n",
    "    #convert to snakelist\n",
    "    cnn_snakelist = SnakeList()\n",
    "    for snakei in all_traces:\n",
    "        cnn_snake = Snake()\n",
    "        for x,y,z,r in snakei:\n",
    "            cnn_snake.addSWC(Point3D(x,y,z),r)\n",
    "        cnn_snake = cnn_snake.trimRange(icafem.box)\n",
    "        cnn_snakelist.addSnake(cnn_snake.resampleSnake(1))\n",
    "\n",
    "    cnn_snakelist = cnn_snakelist.trimDuplicateSnake(icafem.shape)\n",
    "    icafem.writeSWC(swc_name,cnn_snakelist)\n",
    "    print(cnn_snakelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#search vessel to ostias based on relation in root\n",
    "if 0:\n",
    "    single_tree = dfs_search_tree(root)\n",
    "    vessel_tree_postprocess = []\n",
    "    for vessel_list in single_tree:\n",
    "        vessel_list.pop(0)\n",
    "        res = np.array([]).reshape(0, 4)\n",
    "        while vessel_list:\n",
    "            first_node = vessel_list[0]\n",
    "            first_res = np.hstack((first_node.value,first_node.rad))\n",
    "            vessel_list.pop(0)\n",
    "            if vessel_list:\n",
    "                second_node = vessel_list[0]\n",
    "                first_res = first_res[:second_node.start_point_index]\n",
    "                res = np.vstack((res, first_res))\n",
    "            else:\n",
    "                res = np.vstack((res, first_res))\n",
    "                vessel_tree_postprocess.append(res)\n",
    "    cnn_snakelist = SnakeList()\n",
    "    for snakei in vessel_tree_postprocess:\n",
    "        cnn_snake = Snake()\n",
    "        for x,y,z,r in snakei:\n",
    "            cnn_snake.addSWC(Point3D(x,y,z),r)\n",
    "        cnn_snakelist.addSnake(cnn_snake.resampleSnake(1))\n",
    "    cnn_snakelist.trimDuplicateSnake(icafem.shape)\n",
    "    icafem.writeSWC('cnn_ves_snake',cnn_snakelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icafem = iCafe(icafe_dir+'/'+pilist[1])\n",
    "icafem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image\n",
    "import SimpleITK as sitk\n",
    "file_name = icafem.getPath('o')\n",
    "re_spacing_img = sitk.GetArrayFromImage(sitk.ReadImage(file_name))\n",
    "icafem,re_spacing_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iCafePython.connect.ext import extSnake\n",
    "seg_ves_snakelist = SnakeList()\n",
    "csnake = Snake()\n",
    "csnake.addSWC(Point3D(424, 40, 406),1)\n",
    "csnake.addSWC(Point3D(423, 41, 410),1)\n",
    "seg_ves_snakelist.addSnake(csnake)\n",
    "seg_ves_ext_snakelist = extSnake(seg_ves_snakelist,infer_model,re_spacing_img,DEBUG=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iCafePython.connect.ext import createLabelMap\n",
    "traced_img = createLabelMap(seg_ves_snakelist, re_spacing_img.shape)\n",
    "snakei = 195\n",
    "seg_ves_snake = seg_ves_snakelist[snakei]\n",
    "chead = seg_ves_snake[0].pos.lst()\n",
    "move_dir_head = (seg_ves_snake[0].pos - seg_ves_snake[1].pos).norm()\n",
    "\n",
    "ctail = seg_ves_snake[-1].pos.lst()\n",
    "move_dir_tail = (seg_ves_snake[-1].pos - seg_ves_snake[-2].pos).norm()\n",
    "\n",
    "# add seg ahead of head of snake\n",
    "# set starting point as -1 to allow stretching\n",
    "paint_ball(traced_img, seg_ves_snake[0].pos, seg_ves_snake[0].rad, -1)\n",
    "add_seg = extend_seg_end(infer_model, re_spacing_img, traced_img, chead, move_dir_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from iCafePython.connect.ext import extSnake\n",
    "seg_ves_snakelist = icafem.readSnake('seg_ves')\n",
    "seg_ves_ext_snakelist = extSnake(seg_ves_snakelist,infer_model,re_spacing_img,DEBUG=0)\n",
    "icafem.writeSWC('seg_ves_ext',seg_ves_ext_snakelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#coronary original model\n",
    "checkpoint_path_infer = r\"U:\\LiChen\\AICafe\\CNNTracker\\checkpoint\\classification_checkpoints\\centerline_net_model_s1.pkl\"\n",
    "max_points = 500\n",
    "prob_thr = 0.85\n",
    "infer_model = CenterlineNet(n_classes=max_points)\n",
    "checkpoint = torch.load(checkpoint_path_infer)\n",
    "net_dict = checkpoint['net_dict']\n",
    "infer_model.load_state_dict(net_dict)\n",
    "infer_model.to(device)\n",
    "infer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'U:\\LiChen\\AICafe\\CNNTracker\\infer_tools_tree')\n",
    "from infer_tools_tree.build_vessel_tree import build_vessel_tree, TreeNode, dfs_search_tree, search_first_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#load image\n",
    "import SimpleITK as sitk\n",
    "file_name = icafem.getPath('o')\n",
    "re_spacing_img = sitk.GetArrayFromImage(sitk.ReadImage(file_name))\n",
    "re_spacing_img.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paint_ball(img_fill, ct, rad, target=0):\n",
    "    ct_int = ct.intlst()\n",
    "    rad_int = int(round(rad))\n",
    "    posz = min(img_fill.shape[0]-1,ct_int[2])\n",
    "    posx = min(img_fill.shape[1]-1,ct_int[0])\n",
    "    posy = min(img_fill.shape[2]-1,ct_int[1])\n",
    "    img_fill[posz,posx,posy] = 0\n",
    "    for ofx in range(-rad_int, rad_int):\n",
    "        for ofy in range(-rad_int, rad_int):\n",
    "            for ofz in range(-rad_int, rad_int):\n",
    "                cpos = Point3D([ct_int[0] + ofx, ct_int[1] + ofy, ct_int[2] + ofz])\n",
    "                cpos.boundList([img_fill.shape[1],img_fill.shape[2],img_fill.shape[0]])\n",
    "                cdist = cpos.dist(ct)\n",
    "                if cdist > rad:\n",
    "                    continue\n",
    "                posz = min(img_fill.shape[0]-1,cpos.intlst()[2])\n",
    "                posx = min(img_fill.shape[1]-1,cpos.intlst()[0])\n",
    "                posy = min(img_fill.shape[2]-1,cpos.intlst()[1])\n",
    "                img_fill[posz,posx,posy] = target\n",
    "                    \n",
    "def createLabelMap(seg_ves_snakelist,shape):\n",
    "    label_img = np.ones(shape, dtype=np.int16)*(-1)\n",
    "    for snakeid in range(seg_ves_snakelist.NSnakes):\n",
    "        if snakeid%5==0:\n",
    "            print('\\rpainting snake',snakeid,end='')\n",
    "        for pti in range(seg_ves_snakelist._snakelist[snakeid].NP):\n",
    "            pos = seg_ves_snakelist._snakelist[snakeid][pti].pos\n",
    "            rad = seg_ves_snakelist._snakelist[snakeid][pti].rad\n",
    "            # paint within radius at pos position\n",
    "            paint_ball(label_img, pos, rad, 0)\n",
    "    return label_img\n",
    "\n",
    "from infer_tools_tree.build_vessel_tree import search_first_node, search_one_direction\n",
    "def extend_seg_end(infer_model, re_spacing_img, traced_img, chead, cdir_mv):\n",
    "    #chead: current segment ending position\n",
    "    #cdir_mv: move direction based on first two pts on the original segment\n",
    "    prob_records=[0,0,0]\n",
    "    res = search_first_node(infer_model, re_spacing_img, start=chead, prob_records=prob_records)\n",
    "    add_seg = Snake()\n",
    "    if res is None:\n",
    "        print('not valid patch')\n",
    "    else:\n",
    "        direction, prob_records, curr_r = res\n",
    "        if prob_records[-1]>prob_thr:\n",
    "            print('head prob',prob_records[-1],'higher than termination prob_thr',prob_thr)\n",
    "        else:\n",
    "            forward_matchness = Point3D(direction['forward_vector']).norm()*cdir_mv\n",
    "            backward_matchness = Point3D(direction['backward_vector']).norm()*cdir_mv\n",
    "            if forward_matchness>backward_matchness:\n",
    "                sel_start_pos = direction['forward']\n",
    "                sel_direction = direction['forward_vector']\n",
    "            else:\n",
    "                sel_start_pos = direction['backward']\n",
    "                sel_direction = direction['backward_vector']\n",
    "\n",
    "            point_list = []\n",
    "            r_list = []\n",
    "            \n",
    "            find_node = search_one_direction(infer_model, traced_img, re_spacing_img, start=sel_start_pos,\n",
    "                                                 move_direction=sel_direction,\n",
    "                                                 prob_records=prob_records,\n",
    "                                                 r_list=r_list, point_list=point_list,step_ratio=0.5)\n",
    "            for pti in range(len(point_list)):\n",
    "                add_seg.addSWC(Point3D(point_list[pti]),r_list[pti])\n",
    "    return add_seg        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.max(traced_img,axis=0))\n",
    "plt.savefig('1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "seg_ves_snakelist = icafem.readSnake('seg_ves')\n",
    "#traced_img = seg_ves_snakelist.labelMap(icafem.shape)\n",
    "traced_img = createLabelMap(seg_ves_snakelist,re_spacing_img.shape)\n",
    "\n",
    "for snakei in range(seg_ves_snakelist.NSnakes):\n",
    "    seg_ves_snake = seg_ves_snakelist[snakei]\n",
    "    chead = seg_ves_snake[0].pos.lst()\n",
    "    move_dir_head = (seg_ves_snake[0].pos-seg_ves_snake[1].pos).norm()\n",
    "\n",
    "    ctail = seg_ves_snake[-1].pos.lst()\n",
    "    move_dir_tail = (seg_ves_snake[-1].pos-seg_ves_snake[-2].pos).norm()\n",
    "    \n",
    "    #add seg ahead of head of snake\n",
    "    #set starting point as -1 to allow stretching\n",
    "    paint_ball(traced_img, seg_ves_snake[0].pos, seg_ves_snake[0].rad, -1)\n",
    "    add_seg = extend_seg_end(infer_model, re_spacing_img, traced_img, chead, move_dir_head)\n",
    "    #restore\n",
    "    paint_ball(traced_img, seg_ves_snake[0].pos, seg_ves_snake[0].rad, 0)\n",
    "    if add_seg.NP>0:\n",
    "        print(snakei,'head prepend',add_seg)\n",
    "        seg_ves_snake.mergeSnake(add_seg,reverse=False,append=False)\n",
    "    else:\n",
    "        print(snakei,'head no extend')\n",
    "    #add seg after tail of snake\n",
    "    paint_ball(traced_img, seg_ves_snake[-1].pos, seg_ves_snake[-1].rad, -1)\n",
    "    add_seg = extend_seg_end(infer_model, re_spacing_img, traced_img, ctail, move_dir_tail)\n",
    "    paint_ball(traced_img, seg_ves_snake[-1].pos, seg_ves_snake[-1].rad, 0)\n",
    "    if add_seg.NP>0:\n",
    "        print(snakei,'tail add',add_seg)\n",
    "        seg_ves_snake.mergeSnake(add_seg,reverse=False,append=True)\n",
    "    else:\n",
    "        print(snakei,'tail no extend')\n",
    "        \n",
    "icafem.writeSWC('seg_ves_ext',seg_ves_snakelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "seg_ves_snakelist = icafem.readSnake('seg_ves')\n",
    "#traced_img = seg_ves_snakelist.labelMap(icafem.shape)\n",
    "traced_img = createLabelMap(seg_ves_snakelist,re_spacing_img.shape)\n",
    "\n",
    "\n",
    "seg_ves_snakelist = icafem.readSnake('seg_ves')\n",
    "snakei = 175\n",
    "seg_ves_snake = seg_ves_snakelist[snakei]\n",
    "\n",
    "if 1:\n",
    "    chead = seg_ves_snake[0].pos.lst()\n",
    "\n",
    "    move_dir_head = (seg_ves_snake[1].pos-seg_ves_snake[0].pos).norm()\n",
    "\n",
    "    paint_ball(traced_img, seg_ves_snake[0].pos, seg_ves_snake[0].rad, -1)\n",
    "\n",
    "    prob_records=[0,0,0]\n",
    "    res = search_first_node(infer_model, re_spacing_img, start=chead, prob_records=prob_records)\n",
    "    direction, prob_records, curr_r = res\n",
    "    print('prob',prob_records[-1])\n",
    "\n",
    "    cdir_mv = move_dir_head\n",
    "    \n",
    "else:\n",
    "    \n",
    "    ctail = seg_ves_snake[-1].pos.lst()\n",
    "    move_dir_tail = (seg_ves_snake[-1].pos-seg_ves_snake[-2].pos).norm()\n",
    "\n",
    "    paint_ball(traced_img, seg_ves_snake[-1].pos, seg_ves_snake[-1].rad, -1)\n",
    "\n",
    "    prob_records=[0,0,0]\n",
    "    res = search_first_node(infer_model, re_spacing_img, start=ctail, prob_records=prob_records)\n",
    "    direction, prob_records, curr_r = res\n",
    "    print('prob',prob_records[-1])\n",
    "    cdir_mv = move_dir_tail\n",
    "\n",
    "forward_matchness = Point3D(direction['forward_vector']).norm()*cdir_mv\n",
    "backward_matchness = Point3D(direction['backward_vector']).norm()*cdir_mv\n",
    "\n",
    "print(forward_matchness,backward_matchness)\n",
    "if forward_matchness>backward_matchness:\n",
    "    sel_start_pos = direction['forward']\n",
    "    sel_direction = direction['forward_vector']\n",
    "else:\n",
    "    sel_start_pos = direction['backward']\n",
    "    sel_direction = direction['backward_vector']\n",
    "print(sel_start_pos,'sel pos')\n",
    "\n",
    "point_list = []\n",
    "r_list = []\n",
    "\n",
    "find_node = search_one_direction(infer_model, traced_img, re_spacing_img, start=sel_start_pos,\n",
    "                                     move_direction=sel_direction,\n",
    "                                     prob_records=prob_records,\n",
    "                                     r_list=r_list, point_list=point_list)\n",
    "\n",
    "len(point_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AICafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'BRAVEAI'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "seg_model_name = 'LumenSeg2-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dbname = 'RotterdanCoronary'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "pilist = ['0_dataset05_U']\n",
    "seg_model_name = 'CoronarySeg1-8-5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'UNC'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "seg_model_name = 'LumenSeg5-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'CAREIIMERGEGT'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "seg_model_name = 'LumenSeg6-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'IPH-Sup-TOF-FullCoverage'\n",
    "icafe_dir = r'\\\\DESKTOP2\\GiCafe\\result/'+dbname\n",
    "seg_model_name = 'LumenSeg7-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from s.whole.modelname\n",
    "from iCafePython.connect.ext import extSnake\n",
    "import SimpleITK as sitk\n",
    "\n",
    "RETRACE = 1\n",
    "RETREE = 1\n",
    "\n",
    "seg_src = 's.whole.'+seg_model_name\n",
    "DEBUG = 0\n",
    "\n",
    "#max search range in merge/branch, unit in mm\n",
    "search_range_thres = 10\n",
    "\n",
    "for pi in pilist[1:]:\n",
    "    print('='*10,'Start processing',pilist.index(pi),'/',len(pilist),pi,'='*10)\n",
    "    if not os.path.exists(icafe_dir+'/'+pi):\n",
    "        os.mkdir(icafe_dir+'/'+pi)\n",
    "        \n",
    "    icafem = iCafe(icafe_dir+'/'+pi)\n",
    "    \n",
    "    icafem.loadImg(seg_src)\n",
    "    icafem.saveImg('s.whole',icafem.I[seg_src],np.float16)\n",
    "    icafem.loadImg('s.whole')\n",
    "\n",
    "    #export v.tif for 3d visualization\n",
    "    if 'v' not in icafem.listAvailImgs():\n",
    "        vimg = copy.copy(icafem.I['s.whole'])\n",
    "        vimg[vimg<0] = 0\n",
    "        vimg = (vimg*255).astype(np.uint16)\n",
    "        icafem.saveImg('v',vimg,np.int16)\n",
    "        \n",
    "        \n",
    "    #combined operations\n",
    "    if RETRACE or not icafem.existPath('seg_ves_ext.swc'):\n",
    "        if 's.whole' not in icafem.I:\n",
    "            icafem.loadImg('s.whole')\n",
    "        seg_ves_snakelist = icafem.constructSkeleton(icafem.I['s.whole']>0.3)\n",
    "        \n",
    "        #load image\n",
    "        file_name = icafem.getPath('o')\n",
    "        re_spacing_img = sitk.GetArrayFromImage(sitk.ReadImage(file_name))\n",
    "\n",
    "        seg_ves_snakelist = icafem.readSnake('seg_ves')\n",
    "        seg_ves_ext_snakelist = extSnake(seg_ves_snakelist,infer_model,re_spacing_img,DEBUG=DEBUG)\n",
    "        icafem.writeSWC('seg_ves_ext',seg_ves_ext_snakelist)\n",
    "    else:\n",
    "        seg_ves_ext_snakelist = icafem.readSnake('seg_ves_ext')\n",
    "        print('read from existing seg ves ext')\n",
    "    if seg_ves_ext_snakelist.NSnakes==0:\n",
    "        print('no snake found in seg ves, abort',pi)\n",
    "        continue\n",
    "    \n",
    "    if RETREE or not icafem.existPath('seg_ves_ext_tree.swc'):\n",
    "        if 's.whole' not in icafem.I:\n",
    "            icafem.loadImg('s.whole')\n",
    "        tree_snakelist = seg_ves_ext_snakelist.tree(icafem,search_range=search_range_thres/icafem.xml.res,int_src='o',DEBUG=DEBUG)\n",
    "        icafem.writeSWC('seg_ves_ext_tree', tree_snakelist)\n",
    "        tree_snakelist = tree_snakelist.tree(icafem,search_range=search_range_thres/3/icafem.xml.res,int_src='s.whole',DEBUG=DEBUG)\n",
    "        icafem.writeSWC('seg_ves_ext_tree2', tree_snakelist)\n",
    "        \n",
    "        tree_main_snakelist = tree_snakelist.mainArtTree(dist_thres=10)\n",
    "        icafem.writeSWC('seg_ves_ext_tree2_main',tree_main_snakelist)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pilist[0]\n",
    "icafem = iCafe(icafe_dir+'/'+pi)\n",
    "_ = icafem.loadImg('s.whole')\n",
    "tree_snakelist = seg_ves_snakelist.tree(icafem,search_range=search_range_thres/icafem.xml.res,int_src='o',DEBUG=DEBUG)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_ves_ext_snakelist = icafem.readSnake('seg_ves_ext')\n",
    "tree_snakelist = seg_ves_ext_snakelist.tree(icafem,search_range=search_range_thres/icafem.xml.res,int_src='o',DEBUG=DEBUG)\n",
    "icafem.writeSWC('seg_ves_ext_tree', tree_snakelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_snakelist.autoBranch()\n",
    "tree_snakelist.autoTransform(mode='length')\n",
    "icafem.writeSWC('seg_ves_ext_tree', tree_snakelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from iCafePython.artlabel.artlabel import ArtLabel\n",
    "art_label_predictor = ArtLabel()\n",
    "\n",
    "for pi in pilist[:1]:\n",
    "    print('='*10,'Start processing',pilist.index(pi),'/',len(pilist),pi,'='*10)\n",
    "    if not os.path.exists(icafe_dir+'/'+dbname+'/'+pi):\n",
    "        os.mkdir(icafe_dir+'/'+dbname+'/'+pi)\n",
    "        \n",
    "    icafem = iCafe(icafe_dir+'/'+dbname+'/'+pi)\n",
    "    \n",
    "    #generate (simplified node!=2) graph for GNN art labeling\n",
    "    G = icafem.generateGraph(graph_ves,None,graphtype='graphsim', mode='test', trim=1)\n",
    "    if len(G.nodes())<5:\n",
    "        print('too few snakes for artlabeling')\n",
    "        continue\n",
    "    icafem.writeGraph(G,graphtype='graphsim')\n",
    "\n",
    "    #predict landmarks\n",
    "    pred_landmark, ves_end_pts = art_label_predictor.pred(icafem.getPath('graphsim'),icafem.xml.res)\n",
    "    #complete graph Gcom for finding the pts in the path\n",
    "    Gcom = icafem.generateGraph(graph_ves, None, graphtype='graphcom')\n",
    "    ves_snakelist = findSnakeFromPts(Gcom,G,ves_end_pts)\n",
    "    print('@@@predict',len(pred_landmark),'landmarks',ves_snakelist)\n",
    "    #save landmark and ves\n",
    "    icafem.xml.landmark = pred_landmark\n",
    "    icafem.xml.writexml()\n",
    "    icafem.writeSWC('ves_pred', ves_snakelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sel branch for Harborview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icafem = iCafe(icafe_dir+'/'+'0_ID12_U')\n",
    "seg_ves_snakelist = icafem.readSnake('seg_ves_ext')\n",
    "long_snake_ids = [i for i in range(seg_ves_snakelist.NSnakes) if seg_ves_snakelist[i].length>75]\n",
    "long_snake_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_main_snakelist = seg_ves_snakelist.mainArtTree(dist_thres=10)\n",
    "snake_hash = [tree_main_snakelist[i][0].pos.hashPos()+tree_main_snakelist[i][-1].pos.hashPos() for i in range(tree_main_snakelist.NSnakes)]\n",
    "for sid in long_snake_ids:\n",
    "    if seg_ves_snakelist[sid][0].pos.hashPos()+seg_ves_snakelist[sid][-1].pos.hashPos() in snake_hash:\n",
    "        continue\n",
    "    if seg_ves_snakelist[sid][0].pos.x<75:\n",
    "        continue\n",
    "    tree_main_snakelist.addSnake(seg_ves_snakelist[sid])\n",
    "icafem.writeSWC('tree_main',tree_main_snakelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_simple(snakelist):\n",
    "    snakelist = copy.deepcopy(snakelist)\n",
    "    _ = snakelist.resampleSnakes(1)\n",
    "    #ground truth snakelist from icafem.veslist\n",
    "    all_metic = snakelist.motMetric(icafem.veslist)\n",
    "    metric_dict = all_metic.metrics(['MOTA','IDF1','MOTP','IDS'])\n",
    "    #ref_snakelist = icafem.readSnake('ves')\n",
    "    snakelist.compRefSnakelist(icafem.vessnakelist)\n",
    "    metric_dict['OV'], metric_dict['OF'], metric_dict['OT'], metric_dict['AI'], metric_dict['UM'], metric_dict['UMS'], metric_dict['ref_UM'], metric_dict['ref_UMS'], metric_dict['mean_diff'] = snakelist.evalCompDist()\n",
    "    str = ''\n",
    "    metric_dict_simple = ['MOTA','IDF1','MOTP','IDS','OV']\n",
    "    for key in metric_dict_simple:\n",
    "        str += key+'\\t'\n",
    "    str += '\\n'\n",
    "    for key in metric_dict_simple:\n",
    "        if type(metric_dict[key]) == int:\n",
    "            str += '%d\\t'%metric_dict[key]\n",
    "        else:\n",
    "            str += '%.3f\\t'%metric_dict[key]\n",
    "    print(str)\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pilist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate metric and save in each pi folder\n",
    "REFEAT = 0\n",
    "for pi in pilist[:1]:\n",
    "    print('='*10,'Start processing',pilist.index(pi),'/',len(pilist),pi,'='*10)\n",
    "        \n",
    "    icafem = iCafe(icafe_dir+'/'+pi)\n",
    "    \n",
    "    if REFEAT or not icafem.existPath('metric.pickle'):\n",
    "        print('init metric')\n",
    "        all_metric_dict = {}\n",
    "    else:\n",
    "        print('load metric')\n",
    "        with open(icafem.getPath('metric.pickle'),'rb') as fp:\n",
    "            all_metric_dict = pickle.load(fp)\n",
    "    \n",
    "    for vesname in ['seg_ves_ext_tree2_main']:\n",
    "    #for vesname in ['seg_raw','seg_ves_ext_main','seg_ves_ext_tree2']:\n",
    "    #comparison methods\n",
    "    #for vesname in ['frangi_ves','seg_unet','seg_raw','raw_sep','cnn_snake','dcat_snake','seg_ves_ext_tree2_main']:\n",
    "        #if vesname in all_metric_dict:\n",
    "        #    continue\n",
    "        print('-'*10,vesname,'-'*10)\n",
    "        pred_snakelist = icafem.readSnake(vesname)\n",
    "        if pred_snakelist.NSnakes==0:\n",
    "            print('no snake',pi,vesname)\n",
    "            continue\n",
    "        all_metric_dict[vesname] = eval_simple(pred_snakelist.resampleSnakes(1))\n",
    "        \n",
    "    with open(icafem.getPath('metric.pickle'),'wb') as fp:\n",
    "        pickle.dump(all_metric_dict,fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(icafem.getPath('metric.old.pickle'),'rb') as fp:\n",
    "    all_metric_dict_old = pickle.load(fp)\n",
    "all_metric_dict_old['seg_ves_ext_tree2_main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metric_dict['seg_ves_ext_tree2_main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check feat\n",
    "pi = pilist[0]\n",
    "icafem = iCafe(icafe_dir+'/'+pi)\n",
    "with open(icafem.getPath('metric.pickle'),'rb') as fp:\n",
    "    all_metric_dict = pickle.load(fp)\n",
    "all_metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect feats from pickle\n",
    "eval_vesname = {'frangi_ves':'Frangi','seg_unet':'U-Net','seg_raw':'DDT',\n",
    "                'raw_sep':'iCafe','cnn_snake':'CNN Tracker','dcat_snake':'DCAT','seg_ves_ext_tree2_main':'DOST (ours)',\n",
    "               'seg_ves':'DOST (initial curve)','seg_ves_ext_main':'DOST (deep snake)','seg_ves_ext_tree2':'DOST tree'}\n",
    "feats = {}\n",
    "for vesname in eval_vesname:\n",
    "    feats[vesname] = {}\n",
    "    \n",
    "for pi in pilist[:]:\n",
    "    icafem = iCafe(icafe_dir+'/'+pi)\n",
    "    if not icafem.existPath('metric.pickle'):\n",
    "        continue\n",
    "        \n",
    "    with open(icafem.getPath('metric.pickle'),'rb') as fp:\n",
    "        all_metric_dict = pickle.load(fp)\n",
    "    \n",
    "    #for vesname in all_metric_dict:\n",
    "    for vesname in eval_vesname:\n",
    "        if vesname not in all_metric_dict:\n",
    "            print('no',vesname,'in',pi)\n",
    "            continue\n",
    "        for metric in all_metric_dict[vesname]:\n",
    "            if metric not in feats[vesname]:\n",
    "                feats[vesname][metric] = []\n",
    "            feats[vesname][metric].append(all_metric_dict[vesname][metric]) \n",
    "    \n",
    "\n",
    "sel_metrics = ['OV','AI', 'MOTA', 'IDF1', 'IDS']\n",
    "print('\\t'.join(['']+sel_metrics))\n",
    "for vesname in feats:\n",
    "    featstr = eval_vesname[vesname]+'\\t'\n",
    "    for metric in sel_metrics:\n",
    "        if metric in ['IDS']:\n",
    "            featstr += '%.1f\\t'%np.mean(feats[vesname][metric])            \n",
    "        else:\n",
    "            featstr += '%.3f\\t'%np.mean(feats[vesname][metric])\n",
    "    print(featstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#harborbiew\n",
    "#collect feats from pickle\n",
    "#'frangi_ves':'Frangi','seg_unet':'U-Net','seg_raw':'DDT',\n",
    "                #'raw_sep':'iCafe','cnn_snake':'CNN Tracker','dcat_snake':'DCAT',\n",
    "eval_vesname = {'tree_main':'DOST (ours)'}\n",
    "feats = {}\n",
    "for vesname in eval_vesname:\n",
    "    feats[vesname] = {}\n",
    "    \n",
    "for pi in pilist[:]:\n",
    "    icafem = iCafe(icafe_dir+'/'+pi)\n",
    "    if not icafem.existPath('metric.pickle'):\n",
    "        continue\n",
    "        \n",
    "    with open(icafem.getPath('metric.pickle'),'rb') as fp:\n",
    "        all_metric_dict = pickle.load(fp)\n",
    "    \n",
    "    #for vesname in all_metric_dict:\n",
    "    for vesname in eval_vesname:\n",
    "        if vesname not in all_metric_dict:\n",
    "            print('no',vesname,'in',pi)\n",
    "            continue\n",
    "        for metric in all_metric_dict[vesname]:\n",
    "            if metric not in feats[vesname]:\n",
    "                feats[vesname][metric] = []\n",
    "            feats[vesname][metric].append(all_metric_dict[vesname][metric]) \n",
    "    \n",
    "\n",
    "sel_metrics = ['OV','AI', 'MOTA', 'IDF1', 'IDS']\n",
    "print('\\t'.join(['']+sel_metrics))\n",
    "for vesname in feats:\n",
    "    featstr = eval_vesname[vesname]+'\\t'\n",
    "    for metric in sel_metrics:\n",
    "        if metric in ['IDS']:\n",
    "            featstr += '%.1f\\t'%np.mean(feats[vesname][metric])            \n",
    "        else:\n",
    "            featstr += '%.3f\\t'%np.mean(feats[vesname][metric])\n",
    "    print(featstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats[vesname].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:centerlineNet]",
   "language": "python",
   "name": "conda-env-centerlineNet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
